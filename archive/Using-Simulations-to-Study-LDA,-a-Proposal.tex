%% template.tex
%% from
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

\documentclass[conference,final,]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%

%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )



%% BEGIN MY ADDITIONS %%



\usepackage[unicode=true]{hyperref}

\hypersetup{
            pdftitle={Using Simulations to Study LDA, a Dissertation Proposal},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls

% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}

% Pandoc syntax highlighting


% Pandoc header
\usepackage{bm}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amsfonts}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,mybibfile}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%% END MY ADDITIONS %%


\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Using Simulations to Study LDA, a Dissertation Proposal}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations

\author{

%% ---- classic IEEETrans wide authors' list ----------------
 % -- end affiliation.wide
%% ----------------------------------------------------------



%% ---- classic IEEETrans one column per institution --------
 %% -- beg if/affiliation.institution-columnar
\IEEEauthorblockN{
  %% -- beg for/affiliation.institution.author
Tommy Jones %% -- end for/affiliation.institution.author
}
\IEEEauthorblockA{Computational and Data Sciences\\
George Mason University\\
Fairfax, VA
\\jones.thos.w@gmail.com
}
 %% -- end for/affiliation.institution
 %% -- end if/affiliation.institution-columnar
%% ----------------------------------------------------------





%% ---- one column per author, classic/default IEEETrans ----
 %% -- end if/affiliation.institution-columnar
%% ----------------------------------------------------------

}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
This is an abstract
\end{abstract}

% keywords

% use for special paper notices



% make the title area
\maketitle

% no keywords

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Probabilistic topic models are widely used latent variable models of
language. Popularized in 2002 by latent Dirichlet allocation (LDA)
(Blei, Ng, and Jordan 2003) many related models have been developed, for
example (Blei and Lafferty 2007), (Roberts et al. 2013), (Nguyen et al.
2015), and more. These models share common characteristics and estimate
the probability of topics within contexts and tokens within
topics.\footnote{Technically, probabilistic topic models estimate the
  probability that any token was sampled from a topic given the context
  and the probability of sampling each specific token given the topic,
  respectively.} Even today LDA remains one of the most popular topic
models, and one of the simplest.

Probabilistic topic models have been applied to a variety of tasks.
These tasks include information retreival {[}cite 3{]}, analysis of
historical texts {[}cite 2{]}, machine translation and related tasks
{[}cite 3{]}, and more. In recent years, the machine learning community
has focused more on deep architectures typified by text embeddings
{[}cite 2{]} and pre-train then fine tune transformers {[}cite 3{]}. Yet
probabilistic topic models have remained popular analytical methods in
fields such as computational social science {[}cite 3{]} and the digital
humanities {[}cite 3{]}.\footnote{As we will see in Section 3,
  probabilistic topic models can share similar conceptual frameworks
  with newer methods.}

In spite of their sustained popularity, probabilistic topic models
remain challenging to use. Some of these challenges are conceptual.
Probabilistic topic models have user-set tuning parameters, called
``hyperparameters'' in the machine learning literature, whose optimal
settings are not obvious. Moreover, because probabilistic topic models
estimate parameters for a process that is \emph{not} how people write.
Because of this, there is no ground truth against which to compare
models for a sense of ``correctness'' that researchers can use to
develop modeling strategies and metrics to guard against pathological
misspecification.

In some cases, challenges are more practical. Software implementing
probabilistic topic models can be challenging to use and offer limited
functionality. In particular, those that employ probabilistic topic
models in industry often have a need to update models based on new or
updated data. To date, there has been little research on transfer
learning for probabilistic topic models. No off-the-shelf software
implements such a paradigm. The result is that applied practicioners
face an unpleasant tradeoff. Either models go stale or they must be
re-trained from scratch. In the former case, innacuracies creep in over
time. In the latter case, topics are re-initialized at random, breaking
continuity with the old model.

What is more, transfer learning is becoming paramount to modern machine
learning for natural language. The last few years have seen an explosion
of ``transformer'' models which rely on a paradigm of training a
``base'' model on an unsupervised or semi-supervised task. These base
models tend to use as much language data as possible. Then the base
model is transfered to a smaller dataset on a narrow supervised task.
The result has been an impressive increase in performance on many
standard NLP benchmarks. No such paradigm exists for probabilistic topic
models.

In an attempt to address these shortcomings, I propose three research
studies, each building on the last. In each, I will focus on Latent
Dirichlet Allocation (LDA) for its simplicity and popularity. LDA is
closely related to other probabilistic topic models. This enables a
natural extension of this research to other probabilistic topic models.

The first study relates some empirical laws of language to LDA as a
generative process. This enables a principled method for conducting
simulation studies of LDA. Simulation studies are a natural means for
imposing a sense of ``correctness'' in studying statistical
models.{[}cite simulation study tutorial{]} Afer linking LDA to
empirical laws of language, this first study will use a combination of
simulations and analytical derivations to address hyperparameter
settings for LDA. The objective is not to develop methods for finding
the ``correct'' model on real data, as no such model exists. Rather it
is to set up guardrails to avoid models that are pathologically
misspecified where an obviously better model does exist.

The second study develops methods for transfer learning in LDA. This
enables the applied practicioner to update models with new data,
preservig continuity with previously-trained models. It also takes a
first step extending LDA towards the state of the art ``pre-train then
fine tune'' paradigm currently popular in natural language processing.

The final study introduces tidylda, a software package for the R
programming language. tidylda integrates into a wider programming
paradigm in the R language known as ``tidy'' programming. It also
implements several novel methods for and related to LDA, including
transfer learning.

The remainder of this document is organized as follows:

\begin{itemize}
\tightlist
\item
  Section 2 gives a brief history of embedding models for text, a
  broader class of models encompassing probabilistic topic models.
\item
  Section 3 re-states the formulation for LDA, compares it to related
  models, and discusses training algorithms for LDA.
\item
  Section 4 explores current approaches for evaluating and studying
  probabilistic topic models, with a focus on LDA.
\item
  Section 5 gives an overview of simulation studies in statistics
  broadly and how they have been applied to probabilistic topic models.
\item
  Section 6 reviews some empirical laws of language that a synthetic
  data set of language must honor to be considered a valid simulation of
  natural language.
\item
  Section 7 outlines the proposed study for developing a principled
  means for simulation studies of LDA.
\item
  Section 8 outlines the proposed study of tranfer learning for LDA.
\item
  Section 9 outlines the proposed study instroducing the tidylda package
  for the R language.
\end{itemize}

Paragraph describing the background of the problem: In spite of
probabilistic topic models having been around for 20 years, stubborn
problems persist that limit their utility.

Paragraph summarizing the (3) studies I plan to do: Demonstrate that

Paragraph describing to whom this dissertation will be of value

\hypertarget{conceptual-framework}{%
\section{Conceptual Framework}\label{conceptual-framework}}

\hypertarget{a-brief-history-of-language-embedding-models}{%
\subsection{A Brief History of Language Embedding
Models}\label{a-brief-history-of-language-embedding-models}}

\hypertarget{lsa-and-matrix-factorization-models}{%
\subsubsection{LSA and Matrix Factorization
Models}\label{lsa-and-matrix-factorization-models}}

Start with LSA (What about term co-occurrence analyses? You have
something on that in textmineR)

\hypertarget{probabilistic-topic-models}{%
\subsubsection{Probabilistic Topic
Models}\label{probabilistic-topic-models}}

\begin{itemize}
\tightlist
\item
  pLSA
\item
  CTM
\item
  STM
\item
  Supervised LDA?
\item
  Hierarchical LDA?
\item
  Dynamic topic models?
\end{itemize}

\hypertarget{word-embedding-models}{%
\subsubsection{Word Embedding Models}\label{word-embedding-models}}

word2vec (and skipgram and negative sampling), doc2vec, GloVe

Describe how traditional topic models fit within this framework document
= ``context''

Word embedding models brought novelty in several ways. First, they
introduced the concept of a word being represented as a distribution,
rather than a binary presence or absence, or a count of occurence in a
downstream model. {[}cite something{]} Second, they introduced the
concept of ``word algebra'' where mathematical operations on word
vectors in the embedding space seem to have semantic interpretations.
{[}cite{]} There is some debate about whether this latter phenomenon is
real or constructed {[}cite{]} but the former has become standard
practice for a wide range of language modeling tasks. Finally, and
perhaps most interestingly, researchers have explored methods for
mapping embeddings in different languages on top of each other allowing
two languages to ``share'' the same semantic space. {[}cite cross
lingual word embeddings{]}

\hypertarget{transformers-and-the-pre-train-then-fine-tune-paradigm}{%
\subsubsection{Transformers and the Pre-Train then Fine-Tune
Paradigm}\label{transformers-and-the-pre-train-then-fine-tune-paradigm}}

Main point: transformer architecture aside: pre-train then fine-tune is
incredibly valuable and maps to a set of problems with traditional topic
models: (a) How to update an existing model with new data without having
to completely re-train from scratch? (b) How to deliver ``big corpus''
linguistic structure to niche problems?

\hypertarget{probabilistic-topic-models-today}{%
\subsection{Probabilistic Topic Models
Today}\label{probabilistic-topic-models-today}}

In spite of the machine learning community shifting its focus neural
networks, probabilistic topic models are still in widespread use.
Frequent users of probabilistic topic models in recent years have come
from political science {[}cite, cite, cite{]} and humanities {[}cite,
cite, cite{]}. These new users, in contrast to the machine learning
community, do not necessarily need more complex models pushing the state
of the art. Instead they need tools that make these models more
accessible and more reliable.

Paragraph emphasizing LDA as an embedding model document = ``context''
embedding to a probability space

\hypertarget{unresolved-issues-with-probabilistic-topic-models}{%
\subsubsection{Unresolved Issues with Probabilistic Topic
Models}\label{unresolved-issues-with-probabilistic-topic-models}}

In {[}cite Boyd-Graber + Mimno{]} Boyd-Graber et al.~cite four areas
that need to be addressed for increased accessibility of probabilistic
topic models. Paraphrasing, these four areas are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The effects of different preprocessing and vocabulary curation steps
  on a resulting model,
\item
  How to think about the different model specification choices a
  researcher must make and the effects of these choices on the resulting
  model,
\item
  Interpreting the results of a probabilistic topic model in a way that
  is meaningful to humans in the context to which it is applied, and
\item
  A systematic investigation of the ways in which topic models can
  assist users' workflows in information oranization and retreival
\end{enumerate}

The research I propose in this document focuses on the second area. In
fact, I argue that the third and fourth follow from the first two. If a
model is pathalogically misspecified, either from missteps in data
curation or in explicit modeling choices, then any interpretation or
application of that model is susptect.

Concretely: {[}Describe 1 and 2 above{]}

Because probabilistic topic models are latent variable models, and they
model a process inconsistent with how humans actually write, there is no
``right'' model for any set of observed data. {[}However, we can
describe the effects of modeling choices in a more rigorous way. And
while there may be no ``right'' model, there are certainly many
``wrong'' ones. A better understanding of these issues can guard against
pathalogical misspecification.{]}

\hypertarget{evaluating-probabilistic-topic-models-today}{%
\subsubsection{Evaluating Probabilistic Topic Models
Today}\label{evaluating-probabilistic-topic-models-today}}

\hypertarget{studying-complex-models-with-stochastic-simulations}{%
\subsection{Studying Complex Models with Stochastic
Simulations}\label{studying-complex-models-with-stochastic-simulations}}

Simulation studies involve gnerating pseudo random data using a known
stochastic process. Simulation studies have a long history in the field
of statistics. {[}cite Ripley 1987, Hoaglin and Andrews 1975, Feiveson
2002{]} The purpose is to study data where the data generating mechanism
is known. ``A key strength of simulation studies is the ability to
understand the behavior of statistical methods because some `truth'
{[}\ldots{}{]} is known from teh process of generating the data.''
{[}cite morris et al.~2019{]}

{[}Establish vocabulary and approach to simulation studies here:
estimator vs estimand, properties, etc.{]}

Probailistic topic models are excellent candidates for study through
simulation. They model an explicit data generating process through
latent variables. The resulting data are observed, but the generating
variables of interest are not. By simulating data through the process
modeled by a topic model, one can obtain a ``ground truth''. One can
then interrogate a topic model as an estimator in terms of desireable
statistical properties and measure its sensitivity to properties of the
input data.

Simulation studies are indeed used to study probabilistic topic models.
These studies usually use synthetic data sets to augment study of real
data sets, rather than studying the synthetic data primarily. They tend
to take one of two forms: either simulated data are drawn from models
specified with priors commonly adopted for model fitting {[}Wallach
dissertation{]} or drawn from the posterior of a model fit on real data
{[}JBG + Mimno{]}.

Yet to be a valid proxy of human language, such simulated data should
have the same gross statistical properties of human language. To this
end, Zipf's law is paramount.

\hypertarget{simulated-corpora-must-have-the-statistical-properties-of-human-language}{%
\subsubsection{Simulated Corpora Must Have the Statistical Properties of
Human
Language}\label{simulated-corpora-must-have-the-statistical-properties-of-human-language}}

\hypertarget{zipfs-law}{%
\paragraph{Zipf's Law}\label{zipfs-law}}

Zipf's law states that the term frequency distribution of any corpus of
language in any language or context, follows a power law distribution.
{[}cite Zipf 1949{]} The conventional wisdom is that this property holds
only for documents of sufficient length and that the lowest-frequency
words in the corpus do not follow the power law. Yet the inclusion of
compound words holds Zipf's law through the lowest frequencies in the
corpus.{[}cite Le Quan Ha et al.{]} In 2011, Goldwater et al. {[}cite{]}
point out that ``it is important for models used in unsupervised
{[}machine{]} learning to be able to describe the gross statistical
properties of the data that they are intended to learn from. Otherwise,
these properties may distort inferences about the parameters of the
model.'' Unfortunately they also note that ``{[}Zipf's law{]} has been
largely ignored in modern machine learning and computational
linguistics.''

\hypertarget{estimating-power-laws}{%
\paragraph{Estimating power laws}\label{estimating-power-laws}}

Summarize some of the literature here. Lean heavily on the poweRlaw
package vignette for source material. (Also cite poweRlaw explicitly)

\hypertarget{zipfs-law-in-the-context-of-lda}{%
\paragraph{Zipf's Law in the Context of
LDA}\label{zipfs-law-in-the-context-of-lda}}

Stochastic simulations for LDA have typically used symmetric priors.
{[}cite cite cite{]} Yet such priors cannot produce corpora that adhere
to Zipf's law.

Goldwater et al.

Cite my proof as appendix

Estimation of Zipf's law can be done emprically from data

More than Zipf's law - Sparsity - Distribution of individual words
across documents should be similar - Heap's law within documents should
be similar(?)

\hypertarget{focus-on-latent-dirichlet-allocation}{%
\subsubsection{Focus on Latent Dirichlet
Allocation}\label{focus-on-latent-dirichlet-allocation}}

2nd simplest and most popoular Probabilistic Topic model.

Simplicity makes it a good candidate for study

Simplicity means simulations have known limitations - e.g.~documents are
independent draws ==\textgreater{} no correlation in topics across
documents - topics are independent draws ==\textgreater{} no correlation
in tokens across topics

We know and expect that in the real world, certain topics should
co-occur across documents and certain tokens should co-occur across
topics.

Nevertheless, I hypothesize that studying LDA with simulations in its
``gold standard'' state may yeild useable insights to guide model
specification on real world data.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\hypertarget{introoverall-approach}{%
\subsection{Intro/overall approach}\label{introoverall-approach}}

Basic idea: use stochastic simulation to study the idealized properties
of LDA for open issues.

First, need to demonstrate that stochastic simulations can produce the
gross statistical properties of human language and, thus, be valid.

\hypertarget{study-a-developing-a-principled-approach-to-performing-simulation-studies-of-lda}{%
\subsection{Study A: Developing a Principled Approach to Performing
Simulation Studies of
LDA}\label{study-a-developing-a-principled-approach-to-performing-simulation-studies-of-lda}}

Hypothesis: Under certain constraints stochastic simulations using the
functional form of LDA can produce synthetic corpora with properties of
real-world corpora.

Need to identify a ``survey'' of languages and tasks to encompass
``language'' broadly.

Goals: 1. Quantify degree and ways that the simulations succeed and fail
2. Look at traditional (and new?) evaluation metrics and their ability
to identify the ``right'' models that generated the data. 3.
Intentionally misspecify models. What damage is done? Can we detect it?
- Related: Does over-specifying the number of topics and then throwing
away ``bad'' topics lead to the correct number vs.~doing grid, random,
or optimized search? 4. Post-hoc identification of stop words? 5.
Effects of document length, number of documents, sampling tokens,
cutting off stop words/infrequent words in model fit

Evaluation metrics to establish simulations: - Correlation and magnitude
of zipf curves - Sparsity - Fraction of ``failed'' KS tests across words
ranked by frequency - Note need to adjust for multiple tests/binomial
proportion gives single thumbs up/down - i.e.~Did we reject more or less
based on expected random chance? - Something about Heap's law? Heap
distributions across documents?

Evaluation metrics for model specification: 1. Usual suspects (and new?)
metrics on both in-sample and held out (i.e.~newly generated) data 2.
Correlation of learned topics to ``true'' topics

Approach: 1. Simulations

Approach: 1. Estimate Zipf from MLE and use for shape of eta 2. Use
Bayesian HP optimization in SigOpt to identify sets of parameter choices
on pareto frontier for each selected real corpus - alpha: shape +
magnitude, eta: magnitude, k - doc lengths are empirical 3. Multiple
simulations using ``optimal'' settings and compare how well simulations
do to real data 4. Next step: look at simulations and see if there are
any patterns to help inform modeling 5. If possible, derive rules to get
``best fit'' models analytically (i.e.~with algebra) 6. Apply rules to
real world data - does it look good? What is the counterfactual?

Something else\ldots{}

\hypertarget{study-b-transfer-learning-for-lda---towards-a-pre-train-then-fine-tune-paradigm}{%
\subsection{Study B: Transfer Learning for LDA - Towards a Pre-train
then Fine Tune
Paradigm}\label{study-b-transfer-learning-for-lda---towards-a-pre-train-then-fine-tune-paradigm}}

Hypothesis: None.

Practical: people need to update models in ways that aren't upsetting to
users New hotness: Pre-train then fine-tune for LDA

\hypertarget{study-c-tidylda---an-r-package-for-lda-topic-modeling-compatible-with-tidy-data-principles}{%
\subsection{Study C: tidylda - An R Package for LDA Topic Modeling
Compatible with Tidy Data
Principles}\label{study-c-tidylda---an-r-package-for-lda-topic-modeling-compatible-with-tidy-data-principles}}

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-blei2007ctm}{}%
Blei, David M., and John D. Lafferty. 2007. ``A correlated topic model
of Science.'' \emph{The Annals of Applied Statistics} 1 (1): 17--35.
\url{https://doi.org/10.1214/07-aoas114}.

\leavevmode\hypertarget{ref-blei2002lda}{}%
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. ``Latent
Dirichlet Allocation.'' \emph{Journal of Machine Learning Research} 3.

\leavevmode\hypertarget{ref-nguyen2015supervisedtm}{}%
Nguyen, Thang, Jordan Boyd-Graber, Jeffrey Lund, Kevin Seppi, and Eric
Ringger. 2015. ``Is Your Anchor Going Up or Down? Fast and Accurate
Supervised Topic Models.'' In \emph{Human Language Technologies: The
2015 Annual Conference of the North American Chapter of the Acl},
746--55. \url{https://doi.org/10.3115/v1/n15-1076}.

\leavevmode\hypertarget{ref-roberts2013stm}{}%
Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, and Edoardo M.
Airoldi. 2013. ``The Structural Topic Model and Applied Social
Science.'' In.

\end{document}


