
# Appendix 2: Expected Term Frequency of the LDA-DGP
Below derives the expected term frequency of a corpus whose terms are generated by the stochastic process modeled by Latent Dirichlet Allocation. This process is

1. Initialize 
  $\\ \boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta)\\$
  $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha)$
2. Then for each document draw
  $\\ n_d \sim \text{Poisson}(\lambda)$
3. Finally, for each document draw the following $n_d$ times
  $\\ z_{d,n} \sim \text{Multinomial}(1, \boldsymbol\theta_d)\\$
  $w_{d,n} \sim \text{Multinomial}(1, \boldsymbol\beta_{z_{d,n}})$

The expected term frequencies of a corpus generated with the above process are proportional to $\boldsymbol\eta$---the parameter for the Dirichlet prior for terms over topics. This implies that for a simulated corpus to follow Zipf's law, then $\boldsymbol\eta$ must be proportional to a power law.

Let's start by carrying the expected value through both sides of equation (11), above, using the law of total expectation.

\begin{align*}
  \mathbb{E}(\mathbf{w}_d) 
    &= \mathbb{E}\left(n_d \odot 
      \boldsymbol\theta_d \cdot \boldsymbol\beta\right)\\
    &= \mathbb{E}\left(n_d
      \begin{pmatrix}
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,1}\\
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,2}\\
        ...\\
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,V}\\
      \end{pmatrix}
    \right)\\
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,1})\\
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,2})\\
        ...\\
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,V})\\
      \end{pmatrix}\\
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \mathbb{E}(\beta_{k,1})\\
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k})\mathbb{E}(\beta_{k,2})\\
        ...\\
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \mathbb{E}(\beta_{k,V})\\
      \end{pmatrix}
\end{align*}

The last step, above, is due to indpendence of $\boldsymbol\theta_d$ and $\boldsymbol\beta_k$ $\forall d,k$.

Before carrying on, let's note two more relationships:

1. $\boldsymbol\beta_k \sim \text{i.i.d. Dirichlet}(\boldsymbol\eta)$ means that $\mathbb{E}(\boldsymbol\beta_i) = \mathbb{E}(\boldsymbol\beta_j)$ $\forall i,j \in \{1,2,..,K\}$
2. The expected value of a Dirchlet random variable---$\mathbf{X}$---with parameter $\boldsymbol\delta$ is $\mathbb{E}(\mathbf{X}) = \frac{1}{\sum_{m=1}^M\delta_m}\cdot\boldsymbol\delta$

From number 1., above, we can pull $\mathbb{E}(\beta_{.,k})$ outside of the summation. And we can carry through the expected values using definition in 2., above.

\begin{align*}
  \mathbb{E}(\mathbf{w}_d) 
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \mathbb{E}(\beta_{k,1}) \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \\
        \mathbb{E}(\beta_{k,2})\sum_{k = 1}^K \mathbb{E}(\theta_{d,k})\\
        ...\\
        \mathbb{E}(\beta_{k,V})\sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} \cdot 1 \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} \cdot 1  \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} \cdot 1  \\
      \end{pmatrix}\\
   &= \frac{n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta \\
   &\propto \boldsymbol\eta
\end{align*}

The end result is that the expected term frequency of a single document is proportional to $\boldsymbol\eta$---the Dirichlet parameter for terms over topics.

The term frequency for the whole corpus is the sum of the term frequencies for each document. Specifically

\begin{align*}
  \mathbf{w} = \sum_{d=1}^D\mathbf{w}_d
\end{align*}

The expected value under the model, then, can be carried through.

\begin{align*}
  \mathbb{E}(\mathbf{w})
    &= \mathbb{E}\left(\sum_{d=1}^D\mathbf{w}_d\right) \\
    &= \sum_{d=1}^D\mathbb{E}(\mathbf{w}_d)\\
    &= \sum_{d=1}^D \frac{n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta\\
    &= \frac{\sum_{d=1}^D n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta\\
    &\propto \boldsymbol\eta
\end{align*}