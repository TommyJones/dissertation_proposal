---
output:
  pdf_document: default
  html_document: default
---

# Appendix 2: Expected Term Frequency of the LDA-DGP
Below derives the expected term frequency of a corpus whose terms are generated by the stochastic process modeled by Latent Dirichlet Allocation, the LDA-DGP. The expected term frequencies of a corpus generated with the above process are proportional to $\boldsymbol\eta$---the parameter for the Dirichlet prior for terms over topics. This implies that for a simulated corpus to follow Zipf's law, then $\boldsymbol\eta$ must be proportional to a power law.

Assuming there are $D$ contexts, $K$ topics, $V$ unique words, $N$ total words and $N_d$ words in the $d$-th context, the LDA-DGP is as follows. For each word, $n$, in context $d$: 

1. Generate $\boldsymbol{B}$ by sampling $K$ topics $\boldsymbol\beta_k \sim \text{Dirichlet}(\boldsymbol\eta), \forall k \in \{1,2,...,K\}$
2. Generate $\boldsymbol\Theta$ by sampling $D$ documents $\boldsymbol\theta_d \sim \text{Dirichlet}(\boldsymbol\alpha), \forall d \in \{1,2, ..., D\}$
3. Then for each context, $d$
    1. Draw topic $z_{d,n}$ from $\text{Multinomial}(\boldsymbol\theta_d)$
    2. Draw word $w_{d,n}$ from $\text{Multinomial}(\boldsymbol\beta_{z_{d,n}})$
    3. Repeat 1. and 2. $N_d$ times.

Note that context $d$ has $N_d$ words $\forall d \in \{1, 2, ..., D\}$ and that the total number of words in the corpus is $N = \sum_{d=1}^D N_d$.

Under the model, the expected term frequency of a single context is

\begin{align}
  \mathbb{E}(\mathbf{w}_d|\boldsymbol\theta_d, \boldsymbol{B})
    &= n_d \odot \boldsymbol\theta_d \cdot \boldsymbol{B}
\end{align}


Using the law of total expectation, we have

\begin{align*}
  \mathbb{E}(\mathbf{w}_d)
    &= \mathbb{E}(\mathbb{E}(\mathbf{w}_d|\boldsymbol\theta_d, \boldsymbol{B}))\\
    &= \mathbb{E}(n_d \odot \boldsymbol\theta_d \cdot \boldsymbol{B})\\
    &= \mathbb{E}\left(n_d \odot 
      \boldsymbol\theta_d \cdot \boldsymbol\beta\right)\\
    &= \mathbb{E}\left(n_d
      \begin{pmatrix}
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,1}\\
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,2}\\
        ...\\
        \sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,V}\\
      \end{pmatrix}
    \right)\\
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,1})\\
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,2})\\
        ...\\
        \mathbb{E}(\sum_{k = 1}^K \theta_{d,k} \cdot \beta_{k,V})\\
      \end{pmatrix}\\
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \mathbb{E}(\beta_{k,1})\\
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k})\mathbb{E}(\beta_{k,2})\\
        ...\\
        \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \mathbb{E}(\beta_{k,V})\\
      \end{pmatrix}
\end{align*}

The last step, above, is due to indpendence of $\boldsymbol\theta_d$ and $\boldsymbol\beta_k$ $\forall d,k$.

Before carrying on, note two more relationships:

1. $\boldsymbol\beta_k \sim \text{i.i.d. Dirichlet}(\boldsymbol\eta)$ means that $\mathbb{E}(\boldsymbol\beta_i) = \mathbb{E}(\boldsymbol\beta_j)$ $\forall i,j \in \{1,2,..,K\}$
2. The expected value of a Dirchlet random variable---$\mathbf{X}$---with parameter $\boldsymbol\delta$ is $\mathbb{E}(\mathbf{X}) = \frac{1}{\sum_{m=1}^M\delta_m}\cdot\boldsymbol\delta$

From number 1., above, we can pull $\mathbb{E}(\beta_{k,.})$ outside of the summation. And we can carry through the expected values using number 2., above.

\begin{align*}
  \mathbb{E}(\mathbf{w}_d) 
    &= \mathbb{E}(n_d)
      \begin{pmatrix}
        \mathbb{E}(\beta_{k,1}) \sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \\
        \mathbb{E}(\beta_{k,2})\sum_{k = 1}^K \mathbb{E}(\theta_{d,k})\\
        ...\\
        \mathbb{E}(\beta_{k,V})\sum_{k = 1}^K \mathbb{E}(\theta_{d,k}) \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} 
          \sum_{k = 1}^K \frac{\alpha_k}{\sum_{k = 1}^K \alpha_k} \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} 
          \frac{\sum_{k = 1}^K \alpha_k}{\sum_{k = 1}^K \alpha_k} \\
      \end{pmatrix}\\
    &= n_d
      \begin{pmatrix}
        \frac{\eta_1}{\sum_{v=1}^V\eta_v} \cdot 1 \\
        \frac{\eta_2}{\sum_{v=1}^V\eta_v} \cdot 1  \\
        ...\\
        \frac{\eta_V}{\sum_{v=1}^V\eta_v} \cdot 1  \\
      \end{pmatrix}\\
   &= \frac{n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta \\
   &\propto \boldsymbol\eta
\end{align*}

The end result is that the expected term frequency of a single document is proportional to $\boldsymbol\eta$---the Dirichlet parameter for terms over topics.

The term frequency for the whole corpus is the sum of the term frequencies for each document. Specifically

\begin{align*}
  \mathbf{w} = \sum_{d=1}^D\mathbf{w}_d
\end{align*}

The expected value under the model, then, can be carried through.

\begin{align*}
  \mathbb{E}(\mathbf{w})
    &= \mathbb{E}\left(\sum_{d=1}^D\mathbf{w}_d\right) \\
    &= \sum_{d=1}^D\mathbb{E}(\mathbf{w}_d)\\
    &= \sum_{d=1}^D \frac{n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta\\
    &= \frac{\sum_{d=1}^D n_d}{\sum_{v=1}^V\eta_v}\boldsymbol\eta\\
    &\propto \boldsymbol\eta
\end{align*}